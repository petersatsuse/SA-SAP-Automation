
== Technology

////
The Technology Layer elements are typically used to model the Technology Architecture of the enterprise, describing the structure and behavior of the technology infrastructure of the enterprise.

* *_How_* various technology components can facilitate this

Technology components utilized as a part of this solution: CSP Specific, Networking, Instance Types, etc.

## Technology (attributes)

#ADOC_ATTRIBUTES+=" --attribute Azure=1"
#ADOC_ATTRIBUTES+=" --attribute instances-Azure=1"
#ADOC_ATTRIBUTES+=" --attribute SBD-Storage-Azure=1"

////

=== Terraform

Terraform along with the specific cloud provider modules is used to create the infrastructure to support the SAP application and supporting services.

The Terraform templates describe everything needed to create the desired infrastructure components. They also provide a range of pre-defined settings to make creating the correct virtual machines, disks, networks etc. simpler.

=== Salt

What is Salt?

Salt is a different approach to infrastructure management, founded on the idea that high-speed communication with large numbers of systems can open up new capabilities. This approach makes Salt a powerful multitasking system that can solve many specific problems in an infrastructure.

The backbone of Salt is the remote execution engine, which creates a high-speed, secure, and bi-directional communication net for groups of systems. On top of this communication system, Salt provides an extremely fast, flexible, and easy-to-use configuration management system called _Salt States_.

An SAP landscape is made up of groups of machines, each machine in the group performing a role. Those groups of machines work in concert with each other to create an application stack.

To effectively manage these groups of machines, an administrator needs to be able to create roles for those groups. As an example, a group of machines that serve front-end web traffic might have roles which indicate that those machines should all have the webserver package installed and that the web service should always be running.

In Salt, the file which contains a mapping between groups of machines on a network and the configuration roles that should be applied to them is called a _top file_.

Top files are named _top.sls_ by default and they are so-named because they always exist in the "top" of a directory hierarchy that contains state files. That directory hierarchy is called a state tree and this is what is used to reference the building blocks for the SAP Landscape.

=== SAP Sizing

To make the SAP sizing more simple SUSE have introduced pre-defined sizes with well-known abbreviation from T-Shirt sizes Small (S), Medium (M), Large (L).

The Small (S) size is targeted at non-productive scenarios, where as the Medium (M) and Large (L) sizes are recommended for production setups, where certified instance types should be used.

It is possible to tweak these pre-defined sizes or create a set of custom settings.

As sizing is critical, it is important to choose the right instance sizes from the cloud provider which are certified by SAP and choose the right number of disks to provide the right I/O, as similar the network throughput for the solution.

=== Building Blocks

The main building blocks for an SAP Landscape are the _Application Layer_, based on Netweaver with SAP Central Services (xSCS) with an Primary Application Server (PAS) and Additional application server (AAS) and the _Database Layer_, which within the Automation project is SAP HANA.

There are two possible models how SAP Business Suite can be deployed, a centralized deployment where all runs on one server or distributed deployment where every service has its own node.

The centralized deployment is mostly used for non-production environments such as sandbox and development environments.

//todo - picture

The distributed deployment is the recommended way for production environments where each of the SAP application layer components is independently installed on different instances. This is the scenario used within the automation project.

//todo - picture


=== High Availability

There are two main parts in an SAP system which should be made highly available in order to achieve less downtime and eliminate single point of failures within the SAP Landscape.

For the Central Services this is Enqueue Replication, and for the database its the HANA System Replication.

Therefore, for each of these building blocks, one additional machine is required to build a two-node cluster within the HA Scenarios.

ifeval::[ "{cloud}" == "Azure" ]

To provide something like a "virtual IP address" which is able to move between the two cluster nodes, we use the _Standard Load Balancer_ service from Azure to provide traffic to only the active node.

image::azure_loadbalancer.png[scaledwidth="80%"]

endif::[]

ifeval::[ "{cloud}" == "AWS" ]

Within an AWS SAP HA Cluster, the HANA Primary and Secondary nodes each reside in 2 different Availability Zones (AZs), therefore to provide an IP address which is portable between the 2 AZs, an AWS Overlay IP address is required. This uses a specific routing entry which can send network traffic to an instance, no matter which Availability Zones (and subnet) the instance is located in.

Overlay IP addresses have one requirement, they need to have a CIDR range outside of the VPC.

It is important to note that this IP address is not externally available, for this the Route53 service should be used. (this is currently not supported by the SUSE SAP Automation framework).

endif::[]

ifeval::[ "{cloud}" == "GCP" ]

Within a Google Cloud SAP HA Cluster, the HANA Primary and Secondary nodes each reside in 2 different Availability Zones (AZs), therefore to provide an IP address which is portable between the 2 AZs, there are two available options:

. A _Standard GCE Load Balancer_ service from Google Cloud to provide traffic to only the active node.
. An _GCE Overlay IP address_. It uses the _gcp-vpc-move-route_ resource agent which can send network traffic to an instance, no matter which Availability Zones (and subnet) the instance is located in.

endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]


=== Additional Services

Depending on the available services from the cloud provider, additional functionality may need to be created as part of the deployment.  i.e. NFS. This is reflected within the top.sls file.


==== NFS service

ifeval::[ "{cloud}" == "Azure" ]

As we started with Azure, there was no NFS service available, so we need to build some with the tools we ship in {sles4sap}. As the NFS service should be high available, we need a second virtual machine to build a two node cluster.

image::Azure_HA_NFS_Service.png[scaledwidth="80%"]

Over the time, Azure provide more and more services. So as of time of writing, there is a native NFS service with help of Netapp available (Azure Netapp files - ANF) and the Azure file service is extending in this direction too.
//todo - link

endif::[]

ifeval::[ "{cloud}" == "AWS" ]
On AWS, Shared SAP resources are managed in AWS Elastic File Systems (EFS). This provides the NFS services required to support the SAP deployment.

endif::[]

ifeval::[ "{cloud}" == "GCP" ]

Currently, we need to build an NFS service with the tools we ship in {sles4sap}. As the NFS service should be highly available, we need two virtual machines to build a two-node cluster.

// GCP image to be added here
//image::Azure_HA_NFS_Service.png[scaledwidth="80%"]

NOTE: Google Cloud provides a native NFS service (Google Cloud Filestore). It is planned to add the support for the Google Cloud Filestore service in the upcoming releases of the SUSE SAP automation platform.

endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]

If the cloud native NFS service is used, no additional virtual machines will be created and the native service need to be set up in advance.

==== Fencing service

For high availability clusters, a mechanism to switch off or reset one machine in the case of a so called "split-brain" is needed. This is used  when the 2 nodes can no longer communicate with each other. This action is called 'fencing'.

There are several methods which can be used depending on the capability of the cloud provider.

ifeval::[ "{cloud}" == "Azure" ]
Microsoft supported the SUSE clustering as first Linux HA solution on Azure. Microsoft and SUSE created a fencing agent for the cluster for Azure. This fencing agent should remove a machine as quickly as possible (immediately) from the cluster, to ensure that there is only one active node. This avoids data corruption.

Initially, the Azure infrastructure only provided a way to gracefully shutdown a machine, which took 10-15 minutes.  This is too long for the split-brain fencing requirement.

To improve on the time taken to fence nodes in Azure, SUSE implemented an OS native mechanism to fence virtual machines. This technology is provided within the SUSE HA Extension and uses storage as an additional communication layer between the nodes. This requires a raw shared disk to provide a mechanism so both nodes can write messages to a central place. It is called SBD - Stonith Block Device or Split Brain Detector.

When originally desinging this solution, the Azure infrastructure did not provide a raw disk service that could be shared between nodes. Therefore, the SBD fencing solution needed to be built with the tools available in the SUSE Linux distribution.  So, with help of an iSCSI server and the Linux watchdog, this method provides fast and reliable SBD fencing.

One additional server is required to provide the iSCSI service.

image::Azure_fence_iscsi.png[scaledwidth="80%"]

// fixme - this is not implemented from the automation as of today
Following recent improvements, there is now a method in the Azure API to "kill" a virtual machine, so that the fencing agent can make use of it and no additional iSCSI machine is needed if the fence agent is used.
The drawback here is, in order to talk to the Azure API, there is a need for public network connection.

image::Azure_fence_arm.png[scaledwidth="80%"]

So you can choose between two ways

* SDB fencing with help of an iSCSI service
* agent based fencing through API access

There is in the meantime a third option, as Azure provide also raw shared disks as a native service.
As of the time of writing the document, there is only the SBD-fencing mechanism implemented within the automation.
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS Supports the use of the AWS EC2 STONITH mechanism.  This is shipped and supported with the SUSE HA Extension and has been specifically written to fence (poweroff/reboot etc) EC2 instances as part of cluster operations.

Behind the scenes, it uses the AWS CLI, EC2 Tags and IAM as a method to securely identify a node and then fence it.

It requires internet connectivity to ensure the EC2 endpoint can be reached.

A working STONITH method is mandatory to run a supported SUSE cluster on AWS.

endif::[]

ifeval::[ "{cloud}" == "GCP" ]

Google Cloud Supports the use of the GCE STONITH mechanism. This is shipped and supported with the SUSE HA Extension and has been specifically written to fence (poweroff/reboot etc) GCE instances as part of cluster operations.

The GCE fencing mechanism uses the cutting-edge Google Python APIs. There is no longer a need to install the Google Cloud SDK in each HA cluster node to enable the fencing functions.

endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]

IMPORTANT: A working STONITH method is mandatory to run a supported SUSE cluster!

==== Monitoring service

If the Monitoring Service is to be deployed as part of the Automation, an additional virtual machine for this service to support the he Prometheus and Grafana services used to provide this capability.
